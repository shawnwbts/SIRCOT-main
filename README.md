# Context-Aware Smart Contract Comment Generation Using Information Retrieval and Scenario-Driven Chain-of-Thought

This is the source code and dataset to the paper "Context-Aware Smart Contract Comment Generation Using Information Retrieval and Scenario-Driven Chain-of-Thought". Please refer to the paper for the experimental details.

# Approach

![绘图1](https://github.com/shawnwbts/SIRCOT-main/blob/master/SIRCOT.png)

 ## About the Datasets and folder's structure:
In our work, we use data folder to store the data.

In the data folder, we store the data sets after processing, including the training sets and test sets of code, comments.

```
  SIRCOT
│ │
│ ├── baselines(Comments generated by different methods)
│ │ 
│ ├── data(Functions and comments of training and testing sets)
│ │ 
│ ├── evaluation
│ │ ├── S-STexSimEvaluation.py (evaluate S-S Tex.Sim.)	
│ │ ├── S-SSemSimEvaluation.py (evaluate S-S Sem.Sim.)
│ │ ├── S-CSemSimEvaluation (evaluate S-C Sem.Sim.)
│ │
│ ├── model(GraphCodeBert vector)
│ │
│ ├── inspection notes(Score records of three volunteers on https://zenodo.org/records/15093887)  
```

## About the Models

Our work is mainly divided into retrieval model and large language model.

**Notes in retrieval model：**

- "model" directory is generated by GraphCodeBert vector of warehouses and whitening solution of kernel and bias(need to download [GraphCodeBert](https://huggingface.co/microsoft/graphcodebert-base)  on your local hard drive or Online loading from Hugging Face). 
- bert_whitening.py is a file operated with GraphCodeBert+whitening.
- SmartIRCOT.py retrieves few shot <function, comment> pairs.
- BM25Retrieve.py uses the BM25 algorithm for retrieval.
- MLMGraphCodebert.py is used for MLM (Masked Language Modeling) fine-tuning GraphCodeBert, and the fine-tuning results are saved in https://zenodo.org/records/15093887.



**Notes in large language model：**

- COTsLLMs.py is code that modifies the prompt template and invokes the API interface. To use it, you need to install the openAI library in advance, which you can do with the following code.

```
 pip install openai
```

- To run COTsLLMs.py, you need to replace openai.base_url , openai.api_key with your own URL and key.

## How to use our method:

To use our method on a new dataset you need to complete two steps: 1): Retrieve top-k  <function,comment> pairs; 2): Invoke the API to generate comments.

1、run bert_whitening.py

Firstly, run bert_whitening.py to embed the data in the training set.

2、run SmartIRCOT.py 

Second, run SmartIRCOT.py to retrieves few shot <function, comment> pairs. It will output three files to save the reference comments, top-k  <function,comment> pairs.

3、run COTsLLMs.py

Finally, replace openai.base_url , openai.api_key with your own URL and key and put the top-k  <function,comment> pairs in the example_all.csv. The example_all.csv format is as follows:

| code1 | code2 | code3 | code4 | comment1 | comment2 | comment3 | comment4 |
| ----- | ----- | ----- | ----- | -------- | -------- | -------- | -------- |
| xx    | xx    | xx    | xx    | xx       | xx       | xx       | xx       |

You can also modify your own prompt template to conduct further research.

By following these steps, you should be able to replicate our experiments and obtain similar results. Please let us know if you encounter any issues during the process, and we will be happy to assist you.
